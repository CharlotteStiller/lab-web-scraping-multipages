{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-start",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Retrieve-an-arbitrary-Wikipedia-page-of-&quot;Python&quot;-and-create-a-list-of-links-on-that-page\" data-toc-modified-id=\"Retrieve-an-arbitrary-Wikipedia-page-of-&quot;Python&quot;-and-create-a-list-of-links-on-that-page-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page</a></span></li><li><span><a href=\"#Find-the-number-of-titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Find-the-number-of-titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Find the number of titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Create-a-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Create-a-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create a Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Display-the-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Display-the-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>List all language names and number of related articles in the order they appear in <a href=\"wikipedia.org\" target=\"_blank\">wikipedia.org</a></a></span></li><li><span><a href=\"#A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>A list with the different kind of datasets available in <a href=\"data.gov.uk\" target=\"_blank\">data.gov.uk</a></a></span></li><li><span><a href=\"#Display-the-top-10-languages-by-number-of-native-speakers-stored-in-a-pandas-dataframe\" data-toc-modified-id=\"Display-the-top-10-languages-by-number-of-native-speakers-stored-in-a-pandas-dataframe-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Display the top 10 languages by number of native speakers stored in a pandas dataframe</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-poster",
   "metadata": {},
   "source": [
    "As you've seen, scraping the internet is a skill that can get you all sorts of information. Here are some little challenges to gain more experience in the field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-introduction",
   "metadata": {},
   "source": [
    "### Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce3545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appreciated-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url ='https://en.wikipedia.org/wiki/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "376cbc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki_Python: 200\n"
     ]
    }
   ],
   "source": [
    "wiki_python = requests.get(\"https://en.wikipedia.org/wiki/Python\")\n",
    "print(\"Wiki_Python:\", wiki_python.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f01cd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(wiki_python.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "965de773",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_python = soup.find_all(\"a\", href=True, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995bc7d",
   "metadata": {},
   "source": [
    "I tried to build a function to scrap links, but I get a little lost in time. Next time I would change the function, so that I could give different paramentes like \"href\", \"https\" to the function, so it is not only usefull for this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d207bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_python_wiki = []\n",
    "\n",
    "def scraper_links(soup,links_list):\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        url = link.get(\"href\", \"\")\n",
    "        if \"https\" in url:\n",
    "            links_list.append(url)\n",
    "    print(links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffdea825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wiktionary.org/wiki/Python', 'https://en.wiktionary.org/wiki/python', 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0', 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1048703433', 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en', 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452', 'https://commons.wikimedia.org/wiki/Category:Python', 'https://af.wikipedia.org/wiki/Python', 'https://als.wikipedia.org/wiki/Python', 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)', 'https://az.wikipedia.org/wiki/Python', 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)', 'https://be.wikipedia.org/wiki/Python', 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)', 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)', 'https://da.wikipedia.org/wiki/Python', 'https://de.wikipedia.org/wiki/Python', 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)', 'https://eu.wikipedia.org/wiki/Python_(argipena)', 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86', 'https://fr.wikipedia.org/wiki/Python', 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0', 'https://hr.wikipedia.org/wiki/Python_(razdvojba)', 'https://io.wikipedia.org/wiki/Pitono', 'https://id.wikipedia.org/wiki/Python', 'https://ia.wikipedia.org/wiki/Python_(disambiguation)', 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)', 'https://it.wikipedia.org/wiki/Python_(disambigua)', 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F', 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)', 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)', 'https://la.wikipedia.org/wiki/Python_(discretiva)', 'https://lb.wikipedia.org/wiki/Python', 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)', 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)', 'https://nl.wikipedia.org/wiki/Python', 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3', 'https://no.wikipedia.org/wiki/Pyton', 'https://pl.wikipedia.org/wiki/Pyton', 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)', 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)', 'https://sk.wikipedia.org/wiki/Python', 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)', 'https://sh.wikipedia.org/wiki/Python', 'https://fi.wikipedia.org/wiki/Python', 'https://sv.wikipedia.org/wiki/Pyton', 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99', 'https://tr.wikipedia.org/wiki/Python', 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD', 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86', 'https://vi.wikipedia.org/wiki/Python', 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)', 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia', 'https://foundation.wikimedia.org/wiki/Privacy_policy', 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute', 'https://stats.wikimedia.org/#/en.wikipedia.org', 'https://foundation.wikimedia.org/wiki/Cookie_statement', 'https://wikimediafoundation.org/', 'https://www.mediawiki.org/']\n"
     ]
    }
   ],
   "source": [
    "scraper_links(soup,links_python_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c22a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                links\n",
      "0               https://en.wiktionary.org/wiki/Python\n",
      "1               https://en.wiktionary.org/wiki/python\n",
      "2   https://en.wikipedia.org/w/index.php?title=Spe...\n",
      "3   https://en.wikipedia.org/w/index.php?title=Pyt...\n",
      "4   https://donate.wikimedia.org/wiki/Special:Fund...\n",
      "5   https://www.wikidata.org/wiki/Special:EntityPa...\n",
      "6   https://commons.wikimedia.org/wiki/Category:Py...\n",
      "7                https://af.wikipedia.org/wiki/Python\n",
      "8               https://als.wikipedia.org/wiki/Python\n",
      "9   https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%...\n",
      "10               https://az.wikipedia.org/wiki/Python\n",
      "11  https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%...\n",
      "12               https://be.wikipedia.org/wiki/Python\n",
      "13  https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%...\n",
      "14  https://cs.wikipedia.org/wiki/Python_(rozcestn...\n",
      "15               https://da.wikipedia.org/wiki/Python\n",
      "16               https://de.wikipedia.org/wiki/Python\n",
      "17  https://eo.wikipedia.org/wiki/Pitono_(apartigilo)\n",
      "18    https://eu.wikipedia.org/wiki/Python_(argipena)\n",
      "19  https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%...\n",
      "20               https://fr.wikipedia.org/wiki/Python\n",
      "21  https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%...\n",
      "22   https://hr.wikipedia.org/wiki/Python_(razdvojba)\n",
      "23               https://io.wikipedia.org/wiki/Pitono\n",
      "24               https://id.wikipedia.org/wiki/Python\n",
      "25  https://ia.wikipedia.org/wiki/Python_(disambig...\n",
      "26  https://is.wikipedia.org/wiki/Python_(a%C3%B0g...\n",
      "27  https://it.wikipedia.org/wiki/Python_(disambigua)\n",
      "28  https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%...\n",
      "29  https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%...\n",
      "30        https://kg.wikipedia.org/wiki/Mboma_(nyoka)\n",
      "31  https://la.wikipedia.org/wiki/Python_(discretiva)\n",
      "32               https://lb.wikipedia.org/wiki/Python\n",
      "33  https://hu.wikipedia.org/wiki/Python_(egy%C3%A...\n",
      "34  https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%...\n",
      "35               https://nl.wikipedia.org/wiki/Python\n",
      "36  https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%...\n",
      "37                https://no.wikipedia.org/wiki/Pyton\n",
      "38                https://pl.wikipedia.org/wiki/Pyton\n",
      "39  https://pt.wikipedia.org/wiki/Python_(desambig...\n",
      "40  https://ru.wikipedia.org/wiki/Python_(%D0%B7%D...\n",
      "41               https://sk.wikipedia.org/wiki/Python\n",
      "42  https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%...\n",
      "43               https://sh.wikipedia.org/wiki/Python\n",
      "44               https://fi.wikipedia.org/wiki/Python\n",
      "45                https://sv.wikipedia.org/wiki/Pyton\n",
      "46  https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%...\n",
      "47               https://tr.wikipedia.org/wiki/Python\n",
      "48  https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%...\n",
      "49  https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%...\n",
      "50               https://vi.wikipedia.org/wiki/Python\n",
      "51  https://zh.wikipedia.org/wiki/Python_(%E6%B6%8...\n",
      "52  https://www.wikidata.org/wiki/Special:EntityPa...\n",
      "53  https://foundation.wikimedia.org/wiki/Privacy_...\n",
      "54  https://www.mediawiki.org/wiki/Special:MyLangu...\n",
      "55     https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "56  https://foundation.wikimedia.org/wiki/Cookie_s...\n",
      "57                   https://wikimediafoundation.org/\n",
      "58                         https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "links = pd.DataFrame(\n",
    "    {\"links\": links_python_wiki}\n",
    ")\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-performer",
   "metadata": {},
   "source": [
    "### Find the number of titles that have changed in the United States Code since its last release point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "scenic-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52384dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United_States_Code: 200\n"
     ]
    }
   ],
   "source": [
    "United_States_Code = requests.get(url)\n",
    "print(\"United_States_Code:\", United_States_Code.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42270f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(United_States_Code.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5381bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_titles = soup.find_all(\"div\", class_=\"usctitlechanged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02e2ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_text(html, text_list): \n",
    "    for i in html:\n",
    "        text_list.append(i.get_text())\n",
    "    print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33b70523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n          Title 1 - General Provisions ٭\\n', '\\n\\n          Title 5 - Government Organization and Employees ٭\\n', '\\n\\n          Title 8 - Aliens and Nationality\\n\\n        ', '\\n\\n          Title 11 - Bankruptcy ٭\\n', '\\n\\n          Title 18 - Crimes and Criminal Procedure ٭\\n', '\\n\\n          Title 23 - Highways ٭\\n', '\\n\\n          Title 26 - Internal Revenue Code\\n\\n        ', '\\n\\n          Title 42 - The Public Health and Welfare\\n\\n        ']\n"
     ]
    }
   ],
   "source": [
    "scraper_text(changed_titles, changed_titles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "129f57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title 1 - General Provisions ٭', 'Title 5 - Government Organization and Employees ٭', 'Title 8 - Aliens and Nationality', 'Title 11 - Bankruptcy ٭', 'Title 18 - Crimes and Criminal Procedure ٭', 'Title 23 - Highways ٭', 'Title 26 - Internal Revenue Code', 'Title 42 - The Public Health and Welfare']\n"
     ]
    }
   ],
   "source": [
    "print(list(map(str.strip, changed_titles_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d9b5a",
   "metadata": {},
   "source": [
    "There are 8 titles have changed since its last release point. (Title 1, 5, 8, 11, 18, 23, 26, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-necessity",
   "metadata": {},
   "source": [
    "### Create a Python list with the top ten FBI's Most Wanted names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "starting-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBI_most_wanted: 200\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "FBI_most_wanted = requests.get(url)\n",
    "print(\"FBI_most_wanted:\", FBI_most_wanted.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "672e7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(FBI_most_wanted.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "64f2ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FBI_most_wanted_names = soup.find_all(\"h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "edb5fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FBI_most_wanted_names_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a45e44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nJASON DEREK BROWN\\n', '\\nALEXIS FLORES\\n', '\\nJOSE RODOLFO VILLARREAL-HERNANDEZ\\n', '\\nOCTAVIANO JUAREZ-CORRO\\n', '\\nEUGENE PALMER\\n', '\\nRAFAEL CARO-QUINTERO\\n', '\\nBHADRESHKUMAR CHETANBHAI PATEL\\n', '\\nALEJANDRO ROSALES CASTILLO\\n', '\\nROBERT WILLIAM FISHER\\n', '\\nARNOLDO JIMENEZ\\n', 'federal bureau of investigation', '\\nFBI.gov Contact Center\\n']\n"
     ]
    }
   ],
   "source": [
    "scraper_text(FBI_most_wanted_names, FBI_most_wanted_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ffad1e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JASON DEREK BROWN', 'ALEXIS FLORES', 'JOSE RODOLFO VILLARREAL-HERNANDEZ', 'OCTAVIANO JUAREZ-CORRO', 'EUGENE PALMER', 'RAFAEL CARO-QUINTERO', 'BHADRESHKUMAR CHETANBHAI PATEL', 'ALEJANDRO ROSALES CASTILLO', 'ROBERT WILLIAM FISHER', 'ARNOLDO JIMENEZ', 'federal bureau of investigation', 'FBI.gov Contact Center']\n"
     ]
    }
   ],
   "source": [
    "print(list(map(str.strip, FBI_most_wanted_names_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-induction",
   "metadata": {},
   "source": [
    "### Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "copyrighted-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c8ed53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "re = requests.get(url)\n",
    "print(\"Status Code:\", re.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce69e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(re.content, 'html.parser')\n",
    "#<a href=\"/Earthquake/earthquake.php?id=1045716\">2021-10-08&nbsp;&nbsp;&nbsp;16:28:46.0</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "af9fe963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earthquake2021-10-08\\xa0\\xa0\\xa016:33:53.525min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa016:28:46.030min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa016:26:54.032min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa016:26:03.733min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa016:23:52.035min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa016:09:44.349min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:53:38.31hr 05min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:44:39.31hr 14min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:28:00.01hr 31min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:25:24.01hr 33min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:24:34.31hr 34min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:20:48.51hr 38min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:18:08.01hr 40min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa015:08:11.91hr 50min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa014:30:40.92hr 28min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa014:24:02.92hr 35min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa014:23:39.72hr 35min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa014:17:51.22hr 41min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa014:16:13.82hr 42min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa014:09:32.02hr 49min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:51:23.23hr 07min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:51:10.03hr 07min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:31:27.03hr 27min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:30:54.63hr 28min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:27:14.73hr 31min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:26:36.03hr 32min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:25:45.03hr 33min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:24:33.23hr 34min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:24:14.73hr 34min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:23:26.03hr 35min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:22:43.63hr 36min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:20:56.43hr 38min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:20:36.13hr 38min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:19:30.43hr 39min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:17:49.13hr 41min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:11:00.03hr 48min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa013:07:17.03hr 51min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:58:16.64hr 00min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:55:08.84hr 03min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:53:34.04hr 05min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:46:00.34hr 13min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:44:39.14hr 14min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:37:09.04hr 21min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:25:26.24hr 33min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:16:55.54hr 42min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:16:32.24hr 42min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:11:33.64hr 47min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:06:09.44hr 52min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:05:58.04hr 53min ago', 'earthquake2021-10-08\\xa0\\xa0\\xa012:02:22.84hr 56min ago']\n"
     ]
    }
   ],
   "source": [
    "date_time_text = []\n",
    "date_time = soup.find_all(\"td\", class_='tabev6')\n",
    "scraper_text(date_time, date_time_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "61a4b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = soup.find_all(\"td\", class_='tabev6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "22980dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all(\"td\", class_='tabev6'):\n",
    "    if i.has_attr('datetime'):\n",
    "        date_time_text.append(i['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9410be",
   "metadata": {},
   "source": [
    "Have no idea how to give a clean version of this time data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "caa9f47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['46.20\\xa0', '7.31\\xa0', '0.14\\xa0', '124.15\\xa0', '24.16\\xa0', '67.05\\xa0', '28.57\\xa0', '17.84\\xa0', '0.26\\xa0', '119.82\\xa0', '28.56\\xa0', '17.83\\xa0', '19.45\\xa0', '173.48\\xa0', '28.57\\xa0', '17.84\\xa0', '0.33\\xa0', '121.65\\xa0', '8.77\\xa0', '126.46\\xa0', '35.19\\xa0', '25.34\\xa0', '35.02\\xa0', '23.32\\xa0', '30.65\\xa0', '70.98\\xa0', '31.68\\xa0', '104.40\\xa0', '19.17\\xa0', '155.47\\xa0', '19.22\\xa0', '155.42\\xa0', '48.28\\xa0', '122.64\\xa0', '38.99\\xa0', '42.04\\xa0', '35.23\\xa0', '25.32\\xa0', '29.61\\xa0', '116.30\\xa0', '31.68\\xa0', '104.39\\xa0', '1.06\\xa0', '100.06\\xa0', '15.65\\xa0', '71.87\\xa0', '48.15\\xa0', '2.55\\xa0', '35.18\\xa0', '25.31\\xa0', '12.20\\xa0', '87.27\\xa0', '0.67\\xa0', '123.49\\xa0', '28.57\\xa0', '17.85\\xa0', '6.08\\xa0', '151.70\\xa0', '1.36\\xa0', '126.88\\xa0', '44.27\\xa0', '115.01\\xa0', '28.59\\xa0', '17.80\\xa0', '35.56\\xa0', '3.63\\xa0', '28.56\\xa0', '17.84\\xa0', '44.31\\xa0', '115.03\\xa0', '20.86\\xa0', '69.06\\xa0', '8.97\\xa0', '113.80\\xa0', '28.56\\xa0', '17.84\\xa0', '35.12\\xa0', '25.23\\xa0', '28.56\\xa0', '17.83\\xa0', '28.57\\xa0', '17.85\\xa0', '2.00\\xa0', '128.49\\xa0', '28.58\\xa0', '17.83\\xa0', '28.56\\xa0', '17.86\\xa0', '31.67\\xa0', '104.39\\xa0', '36.56\\xa0', '4.67\\xa0', '39.79\\xa0', '41.38\\xa0', '40.64\\xa0', '125.09\\xa0', '27.49\\xa0', '71.01\\xa0', '28.57\\xa0', '17.86\\xa0']\n"
     ]
    }
   ],
   "source": [
    "# latitude\n",
    "latitude_text = []\n",
    "latitude = soup.find_all(\"td\", class_='tabev1')\n",
    "scraper_text(latitude, latitude_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "053dcdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['46.20', '7.31', '0.14', '124.15', '24.16', '67.05', '28.57', '17.84', '0.26', '119.82', '28.56', '17.83', '19.45', '173.48', '28.57', '17.84', '0.33', '121.65', '8.77', '126.46', '35.19', '25.34', '35.02', '23.32', '30.65', '70.98', '31.68', '104.40', '19.17', '155.47', '19.22', '155.42', '48.28', '122.64', '38.99', '42.04', '35.23', '25.32', '29.61', '116.30', '31.68', '104.39', '1.06', '100.06', '15.65', '71.87', '48.15', '2.55', '35.18', '25.31', '12.20', '87.27', '0.67', '123.49', '28.57', '17.85', '6.08', '151.70', '1.36', '126.88', '44.27', '115.01', '28.59', '17.80', '35.56', '3.63', '28.56', '17.84', '44.31', '115.03', '20.86', '69.06', '8.97', '113.80', '28.56', '17.84', '35.12', '25.23', '28.56', '17.83', '28.57', '17.85', '2.00', '128.49', '28.58', '17.83', '28.56', '17.86', '31.67', '104.39', '36.56', '4.67', '39.79', '41.38', '40.64', '125.09', '27.49', '71.01', '28.57', '17.86']\n"
     ]
    }
   ],
   "source": [
    "latitude_text_new =[]\n",
    "for i in latitude_text: \n",
    "    latitude_text_new.append(i.replace('\\xa0', ''))\n",
    "print(latitude_text_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e12e3f",
   "metadata": {},
   "source": [
    "don't now how to seperate longitude and latitude because they have the same tag: tabev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9ca0f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xa0SWITZERLAND', '\\xa0MOLUCCA SEA', '\\xa0SALTA, ARGENTINA', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0MINAHASA, SULAWESI, INDONESIA', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0TONGA', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0SULAWESI, INDONESIA', '\\xa0MINDANAO, PHILIPPINES', '\\xa0CRETE, GREECE', '\\xa0CRETE, GREECE', '\\xa0COQUIMBO, CHILE', '\\xa0WESTERN TEXAS', '\\xa0ISLAND OF HAWAII, HAWAII', '\\xa0ISLAND OF HAWAII, HAWAII', '\\xa0PUGET SOUND REGION, WASHINGTON', '\\xa0EASTERN TURKEY', '\\xa0CRETE, GREECE', '\\xa0OFFSHORE BAJA CALIFORNIA, MEXICO', '\\xa0WESTERN TEXAS', '\\xa0NORTHERN SUMATRA, INDONESIA', '\\xa0SOUTHERN PERU', '\\xa0FRANCE', '\\xa0CRETE, GREECE', '\\xa0NEAR COAST OF NICARAGUA', '\\xa0MINAHASA, SULAWESI, INDONESIA', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0NEW BRITAIN REGION, P.N.G.', '\\xa0MOLUCCA SEA', '\\xa0SOUTHERN IDAHO', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0STRAIT OF GIBRALTAR', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0SOUTHERN IDAHO', '\\xa0TARAPACA, CHILE', '\\xa0JAVA, INDONESIA', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0CRETE, GREECE', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0HALMAHERA, INDONESIA', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0CANARY ISLANDS, SPAIN REGION', '\\xa0WESTERN TEXAS', '\\xa0STRAIT OF GIBRALTAR', '\\xa0EASTERN TURKEY', '\\xa0OFFSHORE NORTHERN CALIFORNIA', '\\xa0OFFSHORE ATACAMA, CHILE', '\\xa0CANARY ISLANDS, SPAIN REGION']\n"
     ]
    }
   ],
   "source": [
    "region_text = []\n",
    "region = soup.find_all(\"td\", class_='tb_region')\n",
    "scraper_text(region, region_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e92b2737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SWITZERLAND', 'MOLUCCA SEA', 'SALTA, ARGENTINA', 'CANARY ISLANDS, SPAIN REGION', 'MINAHASA, SULAWESI, INDONESIA', 'CANARY ISLANDS, SPAIN REGION', 'TONGA', 'CANARY ISLANDS, SPAIN REGION', 'SULAWESI, INDONESIA', 'MINDANAO, PHILIPPINES', 'CRETE, GREECE', 'CRETE, GREECE', 'COQUIMBO, CHILE', 'WESTERN TEXAS', 'ISLAND OF HAWAII, HAWAII', 'ISLAND OF HAWAII, HAWAII', 'PUGET SOUND REGION, WASHINGTON', 'EASTERN TURKEY', 'CRETE, GREECE', 'OFFSHORE BAJA CALIFORNIA, MEXICO', 'WESTERN TEXAS', 'NORTHERN SUMATRA, INDONESIA', 'SOUTHERN PERU', 'FRANCE', 'CRETE, GREECE', 'NEAR COAST OF NICARAGUA', 'MINAHASA, SULAWESI, INDONESIA', 'CANARY ISLANDS, SPAIN REGION', 'NEW BRITAIN REGION, P.N.G.', 'MOLUCCA SEA', 'SOUTHERN IDAHO', 'CANARY ISLANDS, SPAIN REGION', 'STRAIT OF GIBRALTAR', 'CANARY ISLANDS, SPAIN REGION', 'SOUTHERN IDAHO', 'TARAPACA, CHILE', 'JAVA, INDONESIA', 'CANARY ISLANDS, SPAIN REGION', 'CRETE, GREECE', 'CANARY ISLANDS, SPAIN REGION', 'CANARY ISLANDS, SPAIN REGION', 'HALMAHERA, INDONESIA', 'CANARY ISLANDS, SPAIN REGION', 'CANARY ISLANDS, SPAIN REGION', 'WESTERN TEXAS', 'STRAIT OF GIBRALTAR', 'EASTERN TURKEY', 'OFFSHORE NORTHERN CALIFORNIA', 'OFFSHORE ATACAMA, CHILE', 'CANARY ISLANDS, SPAIN REGION']\n"
     ]
    }
   ],
   "source": [
    "region_text_new =[]\n",
    "for i in region_text: \n",
    "    region_text_new.append(i.replace('\\xa0', ''))\n",
    "print(region_text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two list haven't the same length so I can't build an dataframe \n",
    "df = pd.DataFrame(\n",
    "    {\"latitude\": latitude_text_new, \"region\" : region_text_new}\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-defeat",
   "metadata": {},
   "source": [
    "### List all language names and number of related articles in the order they appear in [wikipedia.org](wikipedia.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "metric-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.wikipedia.org/'\n",
    "re = requests.get(url)\n",
    "print(\"Status Code:\", re.status_code)\n",
    "soup = BeautifulSoup(re.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "496692ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Free Encyclopedia', 'English', '日本語', 'Español', 'Deutsch', 'Русский', 'Français', '中文', 'Italiano', 'Português', 'Polski', '\\n\\nDownload Wikipedia for Android or iOS\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "languages_text = []\n",
    "languages = soup.find_all(\"strong\")\n",
    "scraper_text(languages, languages_text)\n",
    "# <strong>English</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4a58a127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " '日本語',\n",
       " 'Español',\n",
       " 'Deutsch',\n",
       " 'Русский',\n",
       " 'Français',\n",
       " '中文',\n",
       " 'Italiano',\n",
       " 'Português',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages_text.remove('\\n\\nDownload Wikipedia for Android or iOS\\n\\n')\n",
    "languages_text.remove('The Free Encyclopedia')\n",
    "languages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "85c44a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6\\xa0383\\xa0000+', '1\\xa0292\\xa0000+', '1\\xa0717\\xa0000+', '2\\xa0617\\xa0000+', '1\\xa0756\\xa0000+', '2\\xa0362\\xa0000+', '1\\xa0231\\xa0000+', '1\\xa0718\\xa0000+', '1\\xa0074\\xa0000+', '1\\xa0490\\xa0000+', '\\n1\\xa0000\\xa0000+\\n', 'العربية', 'مصرى', '\\n100\\xa0000+\\n', 'فارسی', 'עברית', 'قازاقشا', 'تۆرکجه', 'اردو', '\\n10\\xa0000+\\n', 'هَوُسَا', 'كوردی', 'کوردیی ناوەندی', 'مازِرونی', 'پنجابی (شاہ مکھی)', 'پښتو', 'سنڌي', 'ייִדיש', '\\n1\\xa0000+\\n', 'ܐܬܘܪܝܐ', 'گیلکی', 'לאדינו', 'ئۇيغۇرچه', 'ދިވެހިބަސް', '\\n100+\\n', 'كشميري']\n"
     ]
    }
   ],
   "source": [
    "article_text = []\n",
    "article = soup.find_all(\"bdi\")\n",
    "scraper_text(article, article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1c7dff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6383000+', '1292000+', '1717000+', '2617000+', '1756000+', '2362000+', '1231000+', '1718000+', '1074000+', '1490000+', '\\n1000000+\\n', 'العربية', 'مصرى', '\\n100000+\\n', 'فارسی', 'עברית', 'قازاقشا', 'تۆرکجه', 'اردو', '\\n10000+\\n', 'هَوُسَا', 'كوردی', 'کوردیی ناوەندی', 'مازِرونی', 'پنجابی (شاہ مکھی)', 'پښتو', 'سنڌي', 'ייִדיש', '\\n1000+\\n', 'ܐܬܘܪܝܐ', 'گیلکی', 'לאדינו', 'ئۇيغۇرچه', 'ދިވެހިބަސް', '\\n100+\\n', 'كشميري']\n"
     ]
    }
   ],
   "source": [
    "article_text_new =[]\n",
    "for i in article_text: \n",
    "    article_text_new.append(i.replace('\\xa0', ''))\n",
    "print(article_text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5abbaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text_final =['6383000', '1292000', '1717000', '2617000', '1756000', '2362000', '1231000', '1718000', '1074000', '1490000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5f3c5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  articles  languages\n",
      "0  6383000    English\n",
      "1  1292000        日本語\n",
      "2  1717000    Español\n",
      "3  2617000    Deutsch\n",
      "4  1756000    Русский\n",
      "5  2362000   Français\n",
      "6  1231000         中文\n",
      "7  1718000   Italiano\n",
      "8  1074000  Português\n",
      "9  1490000     Polski\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\"articles\": article_text_final, \n",
    "     \"languages\" : languages_text}\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-cartridge",
   "metadata": {},
   "source": [
    "### A list with the different kind of datasets available in [data.gov.uk](data.gov.uk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.gov.uk/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-malpractice",
   "metadata": {},
   "source": [
    "### Display the top 10 languages by number of native speakers stored in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
